{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b624a5e8-d103-46df-85e6-74a7e1df8ffd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n\u001b[1;32m      4\u001b[0m language_detector \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext-classification\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpapluca/xlm-roberta-base-language-detection\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m translator_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/nllb-200-distilled-600M\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "language_detector = pipeline('text-classification', model='papluca/xlm-roberta-base-language-detection')\n",
    "\n",
    "translator_tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "translator_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "\n",
    "rus_toxicity_classifier = pipeline('text-classification', model='cointegrated/rubert-tiny-toxicity')\n",
    "rus_sentiment_classifier = pipeline('sentiment-analysis', model='r1char9/rubert-base-cased-russian-sentiment')\n",
    "\n",
    "en_toxicity_classifier = pipeline('text-classification', model='martin-ha/toxic-comment-model')\n",
    "en_sentiment_classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "zero_shot_classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\")\n",
    "\n",
    "print(\"All models are succesfully loaded!\")\n",
    "\n",
    "SPAM_MARKERS = {\"подпис\", \"канал\", \"заход\", \"переход\", \"профил\", \"ссылк\", \"заработ\", \"крипт\"}\n",
    "URL_PATTERN = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "def is_kazakh(text: str) -> bool:\n",
    "    KAZAKH_CHARS = set(\"ӘәҒғҚқҢңӨөҰұҮүҺһІі\")\n",
    "    return any(char in KAZAKH_CHARS for char in text)\n",
    "\n",
    "def translate_to_russian(text: str, src_lang_code: str = \"kaz_Cyrl\") -> str:\n",
    "    try:\n",
    "        translator_tokenizer.src_lang = src_lang_code\n",
    "        encoded_text = translator_tokenizer(text, return_tensors=\"pt\")\n",
    "        target_lang_id = translator_tokenizer.get_lang_id(\"rus_Cyrl\")\n",
    "        generated_tokens = translator_model.generate(**encoded_text, forced_bos_token_id=target_lang_id)\n",
    "        return translator_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error!: {e}\")\n",
    "        return text\n",
    "\n",
    "def detect_spam_by_rules(text: str) -> bool:\n",
    "    text_lower = text.lower()\n",
    "    if URL_PATTERN.search(text_lower):\n",
    "        return True\n",
    "    if any(marker in text_lower for marker in SPAM_MARKERS):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_moderation_verdict(text: str, language: str) -> str:\n",
    "    if detect_spam_by_rules(text):\n",
    "        return 'spam'\n",
    "    \n",
    "    toxicity_result = 'non-toxic'\n",
    "    if language in ['ru', 'kk']: \n",
    "        toxicity_result = rus_toxicity_classifier(text)[0]['label']\n",
    "    elif language == 'en':\n",
    "        toxicity_result = en_toxicity_classifier(text)[0]['label']\n",
    "    \n",
    "    return 'insult' if toxicity_result == 'toxic' else 'ok'\n",
    "\n",
    "\n",
    "\n",
    "def analyze_comment(comment_text: str) -> dict:\n",
    " \n",
    "    cleaned_text = clean_text(comment_text)\n",
    "    if not cleaned_text:\n",
    "        return {\n",
    "            'text': comment_text, 'language': 'unknown', 'moderation_verdict': 'ok',\n",
    "            'sentiment': 'N/A', 'comment_type': 'N/A'\n",
    "        }\n",
    "\n",
    "    if is_kazakh(cleaned_text):\n",
    "        detected_language = 'kk'\n",
    "    else:\n",
    "        lang_results = language_detector(cleaned_text, top_k=1)\n",
    "        detected_language = lang_results[0]['label']\n",
    "            \n",
    "    sentiment = \"N/A\"\n",
    "    comment_type = \"N/A\"\n",
    "    text_to_analyze = cleaned_text\n",
    "    \n",
    "    if detected_language == 'kk':\n",
    "        text_to_analyze = translate_to_russian(cleaned_text, src_lang_code=\"kaz_Cyrl\")\n",
    "\n",
    "    moderation_verdict = get_moderation_verdict(text_to_analyze, detected_language)\n",
    "\n",
    "    if moderation_verdict != 'spam':\n",
    "        if detected_language in ['ru', 'kk']:\n",
    "            sentiment = rus_sentiment_classifier(text_to_analyze)[0]['label']\n",
    "        elif detected_language == 'en':\n",
    "            sentiment = en_sentiment_classifier(text_to_analyze)[0]['label']\n",
    "        \n",
    "        descriptive_labels = [\n",
    "        \"The user asks a question for telecommunication company,\" \"The user complains about the service for telecommunication company,\"\n",
    "        \"The user expresses gratitude for telecommunication company,\" \"The user shares their opinion for telecommunication company\"\n",
    "        ]\n",
    "        label_map = {\n",
    "            \"The user asks a question for telecommunication company\": \"question\",\n",
    "            \"The user complains about the service for telecommunication company\": \"complaint\",\n",
    "            \"The user expresses gratitude for telecommunication company\": \"gratitude\",\n",
    "            \"The user shares their opinion for telecommunication company\": \"feedback\"\n",
    "        }\n",
    "        \n",
    "        type_result = zero_shot_classifier(text_to_analyze, descriptive_labels)\n",
    "        top_label = type_result['labels'][0]\n",
    "        comment_type = label_map.get(top_label, \"feedback\")\n",
    "\n",
    "    final_analysis = {\n",
    "        'text': comment_text,\n",
    "        'language': detected_language,\n",
    "        'moderation_verdict': moderation_verdict,\n",
    "        'sentiment': sentiment,\n",
    "        'comment_type': comment_type\n",
    "    }\n",
    "    return final_analysis\n",
    "\n",
    "print(\"\\n✅ Analytic module is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5376d58d-fef7-43c4-8115-245e9664f0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "sample_data = {\n",
    "    \"comments\": [\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"author\": \"Анна\",\n",
    "            \"text\": \"Спасибо вам большое, все отлично работает!\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 2,\n",
    "            \"author\": \"Иван\",\n",
    "            \"text\": \"Ужасный интернет, постоянно пропадает!\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 3,\n",
    "            \"author\": \"Spammer\",\n",
    "            \"text\": \"Заходи на мой канал про крипту www.example.com\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 4,\n",
    "            \"author\": \"Марат\",\n",
    "            \"text\": \"Керемет! Маған бәрі ұнады, рахмет сіздерге.\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 5,\n",
    "            \"author\": \"John\",\n",
    "            \"text\": \"Could you please tell me about your new tariffs?\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "INPUT_JSON_PATH = 'comments_input.json'\n",
    "with open(INPUT_JSON_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(sample_data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "print(f\"Создан тестовый файл '{INPUT_JSON_PATH}'\")\n",
    "\n",
    "\n",
    "\n",
    "OUTPUT_JSON_PATH = 'comments_output.json'\n",
    "\n",
    "def process_comments_from_json(input_path: str, output_path: str):\n",
    "    \n",
    "    print(f\"\\nDownlading comments: {input_path}\")\n",
    "    try:\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            comments_to_process = data.get('comments', [])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Input file hasn't been finded in path {input_path}\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"ERROR: Incorrect format JSON in file {input_path}\")\n",
    "        return\n",
    "\n",
    "    if not comments_to_process:\n",
    "        print(\"Couldn't fing comments for analysis.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Finded {len(comments_to_process)} comments. Starting analysis...\")\n",
    "    \n",
    "    results = []\n",
    "    for comment_data in tqdm(comments_to_process, desc=\"Comment analysis\"):\n",
    "        comment_text = comment_data.get('text', '')\n",
    "        \n",
    "        if comment_text:\n",
    "            analysis = analyze_comment(comment_text)\n",
    "            \n",
    "            full_result = {**comment_data, **analysis}\n",
    "            results.append(full_result)\n",
    "        else:\n",
    "            results.append(comment_data)\n",
    "\n",
    "    output_data = {'results': results}\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"\\n✅ Analysis are over. Results saved in file: {output_path}\")\n",
    "\n",
    "process_comments_from_json(INPUT_JSON_PATH, OUTPUT_JSON_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsfs]",
   "language": "python",
   "name": "conda-env-dsfs-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
